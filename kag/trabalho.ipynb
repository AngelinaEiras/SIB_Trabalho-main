{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho SIB\n",
    "\n",
    "Trabalho realizado no âmbito da unidade curricular de Sistemas Inteligentes para a Bioinformática.\n",
    "\n",
    "Grupo 8 constituído por:\n",
    "\n",
    "    - Angelina Eiras PG42861\n",
    "    - Carina Gonçalves PG45466\n",
    "    - Rute Castro PG45475\n",
    "    - Sónia Carvalho PG42882"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter Notebook organizado em secções, que inclua os passos da análise realizada, e que explique muito sucintamente os procedimentos realizados e decisões tomadas ao longo da análise.\n",
    "\n",
    "B2. Kaggle - Novozymes enzyme stability prediction -\n",
    "https://www.kaggle.com/competitions/novozymes-enzyme-stability-prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install git+https://github.com/jcapels/propythia.git@fix_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -e /home/angelina/Desktop/2ano/propythia-fix_dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install propythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade propythia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fase 1:__\n",
    "\n",
    "__Exploração inicial e pré-processamento__\n",
    "- Rever toda a documentação disponível sobre o conjunto de dados.\n",
    "- Carregar o conjunto de dados e realizar uma análise exploratória do mesmo.\n",
    "- Realizar os passos necessários de preparação dos dados e pré-processamento, incluindo possivelmente a geração de atributos, a sua seleção, o tratamento de possíveis valores em falta, etc.\n",
    "\n",
    "# Secção 1:\n",
    "- Descrever e caracterizar os dados atribuídos de acordo com a documentação/literatura existente;\n",
    "- Descrever sucintamente as características dos dados disponíveis a partir da análise exploratória inicial;\n",
    "- Descrever os passos de preparação dos dados e pré-processamento que efetuou, justificando as suas escolhas;\n",
    "- Incluir os gráficos exploratórios iniciais que ilustrem as principais características dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import requests as r\n",
    "from Bio import SeqIO\n",
    "from io import StringIO\n",
    "#import propythia\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import auc, roc_curve, matthews_corrcoef, f1_score, roc_auc_score, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/angelina/Desktop/2ano/SIB_Trabalho-main/kag/train.csv\") #, delimiter=';')\n",
    "df_test = pd.read_csv(\"/home/angelina/Desktop/2ano/SIB_Trabalho-main/kag/test.csv\")\n",
    "df_update = pd.read_csv(\"/home/angelina/Desktop/2ano/SIB_Trabalho-main/kag/train_updates_20220929.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update dos numeros de seq_id no df inicial\n",
    "for i in df_update['seq_id']:\n",
    "    df[df['seq_id'] == i] = df_update[df_update['seq_id'] == i]\n",
    "\n",
    "#nao precisamos dos links\n",
    "df.drop(['data_source'], axis=1, inplace=True)\n",
    "df_test.drop(['data_source'], axis=1, inplace=True)\n",
    "\n",
    "df.dropna(axis=0, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(verbose=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.optimizers.schedules import *\n",
    "from propythia import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gerar descritores\n",
    "\n",
    "from propythia.sequence import ReadSequence\n",
    "from propythia.preprocess import Preprocess\n",
    "#from propythia.descriptors import PeptideDescriptor, Descriptor\n",
    "from propythia.protein_descriptores import ProteinDescritors\n",
    "\n",
    "from propythia.protein_encoding import Encoding\n",
    "\n",
    "descriptors_str = ProteinDescritors(dataset= df ,  col= 'protein_sequence')\n",
    "descriptors_str.get_lenght()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Análise não supervisionada__\n",
    "- Usar as técnicas de redução de dimensionalidade adequadas aos seus dados;\n",
    "- Usar as técnicas de visualização de dados multivariadas adequadas aos seus dados;\n",
    "- Aplicar métodos de clustering que considere adequados aos seus dados.\n",
    "\n",
    "Secção 2:\n",
    "- Reportar/ analisar os resultados obtidos para as técnicas de redução de dimensionalidade e visualização de dados;\n",
    "- Reportar/ analisar os resultados obtidos a partir dos algoritmos de clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aprendizagem máquina__\n",
    "- Comparar o comportamento de diversos modelos/ algoritmos de Aprendizagem Máquina no conjunto de dados. \n",
    "    - Deverá analisar o comportamento dos algoritmos calculando métricas de erro apropriadas e usando métodos de estimação do erro adequado. \n",
    "    - Deverá ainda apresentar o melhor modelo a que consiga chegar para os dados disponíveis, usando todos os exemplos, interpretando-o quando tal for possível.\n",
    "\n",
    "Secção 3: reportar e analisar criticamente os resultados da etapa 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fase 2__\n",
    "\n",
    "__Deep learning__\n",
    "- Utilizar métodos de deep learning de forma semelhante à etapa 3, comparando os resultados com os métodos aí   apresentados.\n",
    "\n",
    "Secção 4: reportar e analisar criticamente os resultados da etapa 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notas:\n",
    "\n",
    "o nosso y é o tm\n",
    "\n",
    "__1º fase__\n",
    "- pré-processamento:\n",
    "    - merge dos datasets (há um com correçoẽs)\n",
    "    - ver composição de amino-ácidos? para se ver há algum tipo de correlação com a tm \n",
    "        - adicionar como features. cross boarding?\n",
    "        - pontes de sal, a.a. com carga, propriedades das proteínas/aa importantes para a tm\n",
    "    - drop de duplicados se tal\n",
    "    - aplicar o package __pip install git+https://github.com/jcapels/propythia.git@fix_dependencies__  para obter descritores fisico-químicos -> para correlações, etc. se valores fora de gama para os modelos (no random forest não é importante, mas modelos lineares é) usar sealers\n",
    "        - à medida que se vão obtendo os descritores guardar, em csv ou o carago\n",
    "    - previsão de estruturas 3D? já não deve dar tempo\n",
    "- exploração dos dados  -> nao supervisionada (k means, pca,...) -> ver quais features com mais correlação com tm\n",
    "    - divisão dos dados com aprd n sup\n",
    "    - comparar com embedding -> pip install bio-embeddings[all]\n",
    "    - split\n",
    "\n",
    "__2º fase__\n",
    "- testar modelos de ML aplicados a regressão\n",
    "- otimizar hiperparametros\n",
    "\n",
    "__3º fase__\n",
    "Encondings na fase DL -> Blosum, ou as proprias features, eu sei lá menino"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
